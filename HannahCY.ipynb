{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- This cell makes the font bigger to make it easy to read. Adjust to taste -->\n",
    "<style>\n",
    ".cell, .CodeMirror pre{ \n",
    "    font-size: 100%;\n",
    "    line-height: 100%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSC470 Assignment 2, 2018\n",
    "\n",
    "## Name: Hannah Clark-Younger\n",
    "## Due Date: Monday September 24th\n",
    "\n",
    "For assignment 2 you need to implement machine learning algorithm(s) to label faces according to:\n",
    "- sex (male/female)\n",
    "- age (child/teen/adult/senior)\n",
    "- expression (smiling/serious)\n",
    "\n",
    "A data set from MIT is made available, along with code to read the images and labels into `numpy` arrays. \n",
    "These arrays are divided into training, validation, and testing data sets.\n",
    "\n",
    "You may use any machine learning algorithms you like to classify the faces.\n",
    "Techniques you may find useful that we've looked at include:\n",
    "- Decision trees and random forests\n",
    "- Boosting (and AdaBoost in particular)\n",
    "- Support Vector Machines (SVMs)\n",
    "- Face detection (to focus on the key parts of the image)\n",
    "- EigenFaces (for dimensionality reduction)\n",
    "- Neural networks in TensorFlow\n",
    "- CNNs in TensorFlow\n",
    "\n",
    "## Submission Requirements\n",
    "\n",
    "You should submit a version of this Notebook renamed to `YourName.ipynb`, so my submission would be `StevenMills.ipynb`. \n",
    "You can assume that the same libraries that are available in the COSC470 Anaconda environment on the lab machines are available.\n",
    "In particular, you can use numpy, scipy, OpenCV, and TensorFlow.\n",
    "\n",
    "I should be able to open your Notebook and run it. The Notebook should contain the code to construct and train your classifier(s) from the training data (using the validation data appropriately) and then to compute the labels of the training data through a call to `computeLabels`, which has a stub implementation at the end of this notebook. \n",
    "\n",
    "## Marking Scheme\n",
    "\n",
    "A rough marking scheme is given below. This is intentionally fairly open, so that I can give you marks for doing good stuff without having to predetermine what stuff is good.\n",
    "\n",
    "- 10 marks for the discussion of choice of algorithms and training strategy\n",
    "- 10 marks for the explanation and clear implementation\n",
    "- 5 marks for performance\n",
    "\n",
    "### Algorithm Choice and Training\n",
    "\n",
    "I will be looking for a description of the algorithm(s) chosen, why you chose that approach, and how you developed, trained and evaluated your method.\n",
    "You should think about issues such as how to best make use of the training and validation data and how to select parameters for your chosen method.\n",
    "\n",
    "You are not restricted to a single classifier or method. If you find it useful to determine age labels first and then use that to help determine expression, then that is fine. If you want to use an SVM for sex classification, but a boosted classifier for age, that's also fine.\n",
    "However, you should discuss why you chose to use the methods you have chosen.\n",
    "\n",
    "### Explanation and Clear Implementation\n",
    "\n",
    "You should implement your chosen algorithm(s) using the training and validation data sets provided. \n",
    "Jupyter notebooks let you interleave discussion and code, so you should clearly describe how your implementation works.\n",
    "You can include mathematics if needed using \\\\(\\LaTeX\\\\)-style markup as demonstrated in the lecture notebooks.\n",
    "I'll be looking for clear implementations that illustrate good practice in training and evaluation.\n",
    "\n",
    "It is expected that you will make appropriate use of libraries such as OpenCV and TensorFlow where appropriate, but your explanation should your understanding of these tools clear. \n",
    "For example, if you choose to use a convolutional network, you should explain your architecture, how it relates to the code, and give some justification for the various parameters that you need to select when making a CNN.\n",
    "\n",
    "### Performance\n",
    "\n",
    "The last cell of the notebook has a function that takes a face data set and produces labels as a result.\n",
    "You should modify this so that it uses your machine learning algorithms to generate the labels.\n",
    "I will then use these labels to compare your results to the ground truth.\n",
    "I may also shuffle the training, validation, and testing data sets around before running your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data Set\n",
    "\n",
    "The following code reads the data into training, testing, and validation sets.\n",
    "It assumes that the `.zip` of labelled face data set from the course website has been unzipped into the same directory as the notebook.\n",
    "There are 1997 training images, and 998 each test and training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read in training data and labels\n",
    "\n",
    "# Some useful parsing functions\n",
    "\n",
    "# male/female -> 0/1\n",
    "def parseSexLabel(string):\n",
    "    if (string.startswith('male')):\n",
    "        return 0\n",
    "    if (string.startswith('female')):\n",
    "        return 1\n",
    "    print(\"ERROR parsing sex from \" + string)\n",
    "\n",
    "\n",
    "# child/teen/adult/senior -> 0/1/2/3\n",
    "def parseAgeLabel(string):\n",
    "    if (string.startswith('child')):\n",
    "        return 0\n",
    "    if (string.startswith('teen')):\n",
    "        return 1\n",
    "    if (string.startswith('adult')):\n",
    "        return 2\n",
    "    if (string.startswith('senior')):\n",
    "        return 3\n",
    "    print(\"ERROR parsing age from \" + string)\n",
    "\n",
    "\n",
    "# serious/smiling -> 0/1\n",
    "def parseExpLabel(string):\n",
    "    if (string.startswith('serious')):\n",
    "        return 0\n",
    "    if (string.startswith('smiling') or string.startswith('funny')):\n",
    "        return 1\n",
    "    print(\"ERROR parsing expression from \" + string)\n",
    "\n",
    "\n",
    "# Count number of training instances\n",
    "\n",
    "numTraining = 0\n",
    "\n",
    "for line in open(\"MITFaces/faceDR\"):\n",
    "    if line.find('_missing descriptor') < 0:\n",
    "        numTraining += 1\n",
    "\n",
    "dimensions = 128 * 128\n",
    "\n",
    "trainingFaces = np.zeros([numTraining, dimensions])\n",
    "trainingSexLabels = np.zeros(numTraining)  # Sex - 0 = male; 1 = female\n",
    "trainingAgeLabels = np.zeros(numTraining)  # Age - 0 = child; 1 = teen; 2 = male\n",
    "trainingExpLabels = np.zeros(numTraining)  # Expression - 0 = serious; 1 = smiling\n",
    "\n",
    "index = 0\n",
    "for line in open(\"MITFaces/faceDR\"):\n",
    "    if line.find('_missing descriptor') >= 0:\n",
    "        continue\n",
    "    # Parse the label data\n",
    "    parts = line.split()\n",
    "    trainingSexLabels[index] = parseSexLabel(parts[2])\n",
    "    trainingAgeLabels[index] = parseAgeLabel(parts[4])\n",
    "    trainingExpLabels[index] = parseExpLabel(parts[8])\n",
    "    # Read in the face\n",
    "    fileName = \"MITFaces/rawdata/\" + parts[0]\n",
    "    fileIn = open(fileName, 'rb')\n",
    "    trainingFaces[index, :] = np.fromfile(fileIn, dtype=np.uint8, count=dimensions) / 255.0\n",
    "    fileIn.close()\n",
    "    # And move along\n",
    "    index += 1\n",
    "\n",
    "# Count number of validation/testing instances\n",
    "\n",
    "numValidation = 0\n",
    "numTesting = 0\n",
    "\n",
    "# Assume they're all Validation\n",
    "for line in open(\"MITFaces/faceDS\"):\n",
    "    if line.find('_missing descriptor') < 0:\n",
    "        numTraining += 1\n",
    "    numValidation += 1\n",
    "\n",
    "# And make half of them testing\n",
    "numTesting = int(numValidation / 2)\n",
    "numValidation -= numTesting\n",
    "\n",
    "validationFaces = np.zeros([numValidation, dimensions])\n",
    "validationSexLabels = np.zeros(numValidation)  # Sex - 0 = male; 1 = female\n",
    "validationAgeLabels = np.zeros(numValidation)  # Age - 0 = child; 1 = teen; 2 = male\n",
    "validationExpLabels = np.zeros(numValidation)  # Expression - 0 = serious; 1 = smiling\n",
    "\n",
    "testingFaces = np.zeros([numTesting, dimensions])\n",
    "testingSexLabels = np.zeros(numTesting)  # Sex - 0 = male; 1 = female\n",
    "testingAgeLabels = np.zeros(numTesting)  # Age - 0 = child; 1 = teen; 2 = male\n",
    "testingExpLabels = np.zeros(numTesting)  # Expression - 0 = serious; 1 = smiling\n",
    "\n",
    "index = 0\n",
    "for line in open(\"MITFaces/faceDS\"):\n",
    "    if line.find('_missing descriptor') >= 0:\n",
    "        continue\n",
    "\n",
    "    # Parse the label data\n",
    "    parts = line.split()\n",
    "    if (index < numTesting):\n",
    "        testingSexLabels[index] = parseSexLabel(parts[2])\n",
    "        testingAgeLabels[index] = parseAgeLabel(parts[4])\n",
    "        testingExpLabels[index] = parseExpLabel(parts[8])\n",
    "        # Read in the face\n",
    "        fileName = \"MITFaces/rawdata/\" + parts[0]\n",
    "        fileIn = open(fileName, 'rb')\n",
    "        testingFaces[index, :] = np.fromfile(fileIn, dtype=np.uint8, count=dimensions) / 255.0\n",
    "        fileIn.close()\n",
    "    else:\n",
    "        vIndex = index - numTesting\n",
    "        validationSexLabels[vIndex] = parseSexLabel(parts[2])\n",
    "        validationAgeLabels[vIndex] = parseAgeLabel(parts[4])\n",
    "        validationExpLabels[vIndex] = parseExpLabel(parts[8])\n",
    "        # Read in the face\n",
    "        fileName = \"MITFaces/rawdata/\" + parts[0]\n",
    "        fileIn = open(fileName, 'rb')\n",
    "        validationFaces[vIndex, :] = np.fromfile(fileIn, dtype=np.uint8, count=dimensions) / 255.0\n",
    "        fileIn.close()\n",
    "\n",
    "    # And move along\n",
    "    index += 1\n",
    "print(\"Data loaded,\", str(trainingFaces.shape[0]), \"training images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP CNN FOR FACE CLASSIFICATION\n",
    "\n",
    "Over the past half-decade or so, Convolutional Neural Networks (CNNs) have consistently been shown to outperform all other methods of machine learning for classification tasks [1] [2] [3] [4]. Given this is a set of three classification tasks, I decided that it was a good bet for this project. CNNs are also simple to implement (in the sense that they don't require complicated engineering) and often generalise well to unseen data after training.\n",
    "\n",
    "The work with CNNs is in picking the hyperparameters: the number of layers, the number of features each layer can detect, the window size of these features, the learning rate and the batch size and so forth. There is no robust theory that instructs us how to do this in a principled way - it is often largely based on trial and error (and experience).\n",
    "\n",
    "Before we launch into the methods used, some analysis of the classification tasks and what would count as 'good' accuracy is necessary. We can consider two kinds of naive classifiers. The first outputs a class at random, and the second has the capacity to learn to output the most common class for all images. In the first case, we should expect 50% accuracy on the Sex and Expression tasks, and 25% accuracy on the Age task. To work out how the second case would perform, we need to know something about the actual distribution of classes. The distribution of the classes over the entire set of images, as well as that over the training, validation, and testing sets I used are given below. It is the testing set that is particularly relevant because that is the set on which the classifiers will be assessed.\n",
    "\n",
    "<table>\n",
    "<tr><th>Sex </th><th>Age</th><th>Expression</th></tr>\n",
    "<tr><td>\n",
    "\n",
    "Class | male | female | \n",
    "--- | --- | --- |\n",
    "Training | 57.6% | 42.4% |\n",
    "Validation | 66.7% | 33.3% |\n",
    "**Testing** | **61.4%** | **38.6%** |\n",
    "Overall | 60.8% | 39.2% |\n",
    "\n",
    "</td><td>\n",
    "    \n",
    "Class | child | teen | adult | senior |\n",
    "--- | --- | --- | --- | --- |\n",
    "Training | 12.2% | 13.1% | 72.0% | 2.7% |\n",
    "Validation | 6.3% | 8.0% | 84.3% | 1.4% |\n",
    "**Testing** | **0.9%** | **0.3%** | **88.7%** | **10.1%** |\n",
    "Overall | 7.9% | 8.6% | 79.3% | 4.2% |\n",
    "\n",
    "</td><td>    \n",
    "      \n",
    "Class | serious | smiling | \n",
    "--- | --- | --- |\n",
    "Training | 45.9% | 54.1% |\n",
    "Validation | 50.0% | 50.0% |\n",
    "**Testing** | **60.1%** | **39.9%** |\n",
    "Overall | 50.5% | 49.5% |\n",
    "\n",
    "</td></tr> </table>\n",
    "\n",
    "The second strategy, then, will result in testing accuracy of 61.4% for the Sex task, 88.7% for the Age task, and 39.9% for the Expression task (because there are more images classified as \"smiling\" in the training set). This means that for the Sex task, we need to improve on 50% to beat the random classifier, and 61.4% to beat the most-common-class classifier. For the Age task, we need to improve on 25% and 88.7% respectively, and for the Expression task we need to improve on 50% and 39.9% respectively.\n",
    "\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "Artificial neural networks, inspired by the mammalian brain, have been shown to be very effective at various kinds of learning tasks, such as classifying images by their content or navigating a robot around the world. After the initial excitement over them in the 1940s to 1960s (see [5] [6] [7]), there was a period referred to as the \"dark ages\" which was spurred by the discovery that, in their simple two-layer form, they are limited to learning tasks that are linearly separable [8]. However, since the conceptual introduction of the generalised delta rule (in 1986) and thus the possibility of having a third (or in fact any number of), \"hidden\", layer(s) [9], the study of neural networks has revived, and indeed exploded.\n",
    "\n",
    "Neural networks are implemented as directed graphs: nodes, or \"neurons,\" with weighted connections. They learn by updating the values of the weights of each connection, which produces different levels of activation in each neuron when it is triggered by some input. Depending on the type of artificial neural network, there may be more restrictions on the architecture - that is, the number and arrangement of these neurons and connections.\n",
    "\n",
    "Deep CNNs have been increasingly popular since 1998 [10], particularly on image classification tasks similar to this one [11] [12]. CNNs are feed-forward (input->layers->output with no loops)neural networks with a particular kind of architecture: they have several layers, including convolutional layers and subsampling layers. The convolutional layers respond to small patches of pixels, rather than to individual pixels. This means that they can preserve the information contained in the relative location of the pixels -- for example, a horizontal line consists of similar coloured pixels lined up next to one another, not randomly scattered around the image. Each convolutional layer looks for a specified number of \"features\" (perhaps horizontal lines, or patches of blue pixels), and it can detect them wherever they may occur in the image. Subsampling (max-pooling is commonly used) layers often follow convolutional layers, and compress or \"pool\" the information extracted by the convolutional layers by keeping track of (in the case of max-pooling) only the strongest candidate for each feature in a given region, or (in the case of average-pooling) the average activation for that feature in the given region. In theory, as the information passes through the network, the features that are detected become more complex. In the earliest layers, the features are basic lines and blobs, and the subsequent layers recombine these features to make increasingly complex features.\n",
    "\n",
    "CNNs often use Rectified Linear Units (ReLU) as the activation function (this defines how the inputs to a given unit are combined to produce the activation of the unit). ReLU is a simple activation function, which takes the input (the sum of the units from the previous layer that are connected to the unit in question, weighted by the trainable weights associated with the respective connections, with the trainable bias of the unit added), and gives as the activation of the unit either this input, or 0, whichever is the highest. This mitigates the vanishing gradient problem because the slope is always 1 (when positive): it enables the weights in earlier layers to train faster than they would if a different activation function were used (sigmoid, for example). All of this means that CNNs are very effective at *learning* useful features, detecting them, and combining them to form representations of types of objects so that they can identify them in images.\n",
    "\n",
    "\n",
    "## My CNN\n",
    "\n",
    "I first implemented a basic CNN with three convolutional and three maxpooling layers. I tried some different numbers of features at each layer, but at its best it reached testing accuracy of 76% for the Sex classification task, 86% for the Age classification task, and 74% for the Expression classification task. While this is significantly above baseline for Sex and Expression (though not for Age), I believed it was possible to do better than that. Since four weeks is not enough time for extensive trial and error, and I don't have much experience, I chose to stand on the shoulders of giants and implement an architecture that was designed by others with much more of both.\n",
    "\n",
    "In recent years, some of the top performing network architectures on similar classification tasks have been AlexNet [1], VGGNet [2], GoogLeNet (Inception) [3], and ResNet[4]. I started with VGGNet (architecture described below), as it is much simpler than its successors, without any of their bells and whistles, such as inception modules and residual modules. As far as vanilla CNNs go, it can be argued that VGGNet is still the best there is. I also implemented a version of ResNet (this can be found at [13]), but I found that it didn't do as well as VGGNet on these classification tasks, while taking significantly longer to train.\n",
    "\n",
    "I trained all my CNNs using the training set (of size 1997), checking the progress by assessing the performance on the validation set after each epoch (each full run through of the training data, with updates to the weights occurring after each mini-batch of size 16). The validation set wasn't used to update weights, just to check its performance on data it isn't training on. When it reached peak performance on validation data, I saved the network's current state and tested it on the testing data, which had previously not been seen by the network at all. Testing was done by running the model and directly getting the accuracy out of it. Alternatively, it can be computed by using the computeLabels method (below) and comparing the predicted labels to the true labels, which is essentially what the model does to compute accuracy anyway. All the results I present are on testing data. Performance on training data easily reached 100% in all cases, and was similar on validation data as on testing data. But it is the testing performance that matters most, because this measures how well the CNN can generalise what it has learned to unfamiliar data. Validation accuracy is what we use to decide when to stop training, so it will be, by definition, at its highest at the point we stop and test.\n",
    "\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "First, I trained my VGGNets (more details below) on the training data as it came. I achieved testing accuracy of 83% (Sex), 91% (Age), and 85% (Exp) (these results given more precisely below). This was notably better than my first attempt at a basic CNN. However, in an attempt to boost performance even more, as the training data set was only 1997 images (very small for CNNs) I decided to try augmenting the training data. In order to increase the size of the training set, as well as the variation among the examples in this set, I augmented each image in three different ways (separately, producing ten versions of each image including the original). Each image was, four separate times, rotated a random amount between -25 and 25 degrees. Each also had noise added, four separate times. Each was also flipped horizontally (this can only be done once). Including the original image as well, that multiplied the training data by 10, giving 19970 training images. I used methods described in [14] to help me do this.\n",
    "\n",
    "Running the next cell performs this data augmentation. You may need to install scikit-image to the working environment for this to work. If this is not possible, this step can be skipped and everything else will still run, and the accuracy will be similar to that given above, and below in Results (pre-augmenting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy import ndarray\n",
    "import skimage as sk ### possibly needs to be installed, not in the existing cosc470 environment\n",
    "from skimage import transform\n",
    "from skimage import util\n",
    "import copy\n",
    "\n",
    "def random_rotation(image_array):\n",
    "    random_degree = random.uniform(-25, 25)\n",
    "    return sk.transform.rotate(image_array, random_degree)\n",
    "\n",
    "def random_noise(image_array):\n",
    "    return sk.util.random_noise(image_array)\n",
    "\n",
    "def horizontal_flip(image_array):\n",
    "    return image_array[:, ::-1]\n",
    "\n",
    "new_faces = np.zeros([trainingFaces.shape[0]*10, dimensions])\n",
    "new_trainingSexLabels = np.zeros(trainingFaces.shape[0]*10)  # Sex - 0 = male; 1 = female\n",
    "new_trainingAgeLabels = np.zeros(trainingFaces.shape[0]*10)  # Age - 0 = child; 1 = teen; 2 = male\n",
    "new_trainingExpLabels = np.zeros(trainingFaces.shape[0]*10)  # Expression - 0 = serious; 1 = smiling\n",
    "\n",
    "for i in range(trainingFaces.shape[0]):\n",
    "    new_index = i*10\n",
    "    new_faces[new_index, :] = copy.deepcopy(trainingFaces[i]) # the original image\n",
    "    reshaped = np.reshape(trainingFaces[i], [128,128])\n",
    "    new_faces[(new_index + 1),:] = np.reshape(random_rotation(reshaped),[128*128]) # the image randomly\n",
    "    new_faces[(new_index + 2),:] = np.reshape(random_rotation(reshaped),[128*128]) # rotated, four times\n",
    "    new_faces[(new_index + 3),:] = np.reshape(random_rotation(reshaped),[128*128])\n",
    "    new_faces[(new_index + 4),:] = np.reshape(random_rotation(reshaped),[128*128])\n",
    "    new_faces[(new_index + 5),:] = random_noise(trainingFaces[i,:]) # the image with\n",
    "    new_faces[(new_index + 6),:] = random_noise(trainingFaces[i,:]) # random noise\n",
    "    new_faces[(new_index + 7),:] = random_noise(trainingFaces[i,:]) # added, four times\n",
    "    new_faces[(new_index + 8),:] = random_noise(trainingFaces[i,:])\n",
    "    new_faces[(new_index + 9),:] = np.reshape(horizontal_flip(reshaped),[128*128]) # horizontally flipped\n",
    "    for j in range(new_index, new_index+10):\n",
    "        new_trainingSexLabels[j] = trainingSexLabels[i]\n",
    "        new_trainingAgeLabels[j] = trainingAgeLabels[i]\n",
    "        new_trainingExpLabels[j] = trainingExpLabels[i]\n",
    "\n",
    "trainingFaces = copy.deepcopy(new_faces)\n",
    "trainingSexLabels = copy.deepcopy(new_trainingSexLabels)\n",
    "trainingAgeLabels = copy.deepcopy(new_trainingAgeLabels)\n",
    "trainingExpLabels = copy.deepcopy(new_trainingExpLabels)\n",
    "print(\"Training data augmented, now\", str(trainingFaces.shape[0]), \"training images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGNet Architecture\n",
    "\n",
    "I implemented the architecture outlined in [2]. They use a stride of 1 for most convolutions so as to maintain the overall size from one to the next, and small conv filters (most are a 3x3 window, some only 1x1). Max-pooling is applied after each small cluster of (2 or 3) convolutional layers, and ReLU is applied on all convolutional and max-pooling layers. Each cluster of layers has the same field of view and the same number of features, and then when max-pooling is applied, a stride of 2 is used to downsize by half, but the number of features is always doubled at these points. The network ends with three fully connected layers, the first two with 4096 features, and the final one with an output size the same as the number of classes (so, 2 for the Sex and Expression tasks, 4 for the Age task). Softmax is applied right at the end to give the actual trainable output: between 0-1 for each class (and they sum to 1), interpretable as the predicted likelihood that the image belongs to that class. The highest of these is taken to be the predicted class for that image.\n",
    "\n",
    "I implemented four different versions of this architecture, three taken directly from [2], and named VGG-C, VGG-D (also known as VGG-16) and VGG-E (also known as VGG-19) following the naming conventions in the original article. The fourth option is VGG-E1, which is VGG-E but with 1x1 convolutional layers, following the pattern of VGG-C. These layers provide a way to increase the non-linearity of the decision function without affecting the receptive fields of the convolutional layers. It is essentially an excuse to use an additional ReLU. This strategy was originally proposed by [14]. So, VGG-E1 is to VGG-E (both 19 layers) as VGG-C is to VGG-D (both 16 layers). All of these options can be chosen by specifying the \"network\" parameter at the top of the next cell. \n",
    "\n",
    "The detail of the respective architectures are as follows (convolutional layers are denoted as \"conv-{window size}-{number of features}\"):\n",
    "\n",
    "| VGG-C        | VGG-D       | VGG-E1  | VGG-E | Name in code |\n",
    "| ------------- |--------| -----|----- | ----- |\n",
    "| conv-3-64    | conv-3-64  | conv-3-64  |conv-3-64 | conv1 |\n",
    "| conv-3-64    | conv-3-64   | conv-3-64  |conv-3-64 | conv2 |\n",
    "| max-pool       | max-pool    |   max-pool |max-pool| max1 |\n",
    "| conv-3-128    | conv-3-128  | conv-3-128  |conv-3-128 | conv3 |\n",
    "| conv-3-128    | conv-3-128   | conv-3-128  |conv-3-128 | conv4 |\n",
    "| max-pool        | max-pool    |   max-pool |max-pool| max2 |\n",
    "| conv-3-256    | conv-3-256  | conv-3-256  |conv-3-256 | conv5 |\n",
    "| conv-3-256    | conv-3-256   | conv-3-256  |conv-3-256 | conv6 |\n",
    "| conv-1-256    | conv-3-256   | conv-3-256  |conv-3-256 | conv7 |\n",
    "|     -          |      -        | conv-1-256  |conv-3-256 | conv75 |\n",
    "| max-pool        | max-pool    |   max-pool |max-pool| max3 |\n",
    "| conv-3-512    | conv-3-512  | conv-3-512  |conv-3-512 | conv8 |\n",
    "| conv-3-512    | conv-3-512   | conv-3-512  |conv-3-512 | conv9 |\n",
    "| conv-1-512    | conv-3-512   | conv-3-512  |conv-3-512 | conv10 |\n",
    "|     -          |      -        | conv-1-512  |conv-3-512 | conv105 |\n",
    "| max-pool        | max-pool    |   max-pool |max-pool| max4 |\n",
    "| conv-3-512    | conv-3-512  | conv-3-512  |conv-3-512 | conv11 |\n",
    "| conv-3-512    | conv-3-512   | conv-3-512  |conv-3-512 | conv12 |\n",
    "| conv-1-512    | conv-3-512   | conv-3-512  |conv-3-512 | conv13 |\n",
    "|     -          |      -        | conv-1-512  |conv-3-512 | conv135 |\n",
    "| max-pool        | max-pool    |   max-pool |max-pool| max5 |\n",
    "| fc-4096        | fc-4096     |   fc-4096  |fc-4096 | fc1 |\n",
    "| fc-4096        | fc-4096     |   fc-4096  |fc-4096 | fc2 |\n",
    "| fc-n_classes        | fc-n_classes      |  fc-n_classes   |fc-n_classes  | fc3 |\n",
    "\n",
    "I also converted the labels to one-hot, which makes it convenient to train as the network gives a softmax prediction (0-1) of how likely it is to be in that class. The predicted class is taken to be the maximum of these activations.\n",
    "\n",
    "The next cell loads the model into the graph. You can choose both the task and which network to train by modifying the *task* and *network* variables. I found that VGG-D and VGG_E tended to be most reliably accurate (results below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "####### MODIFY THESE PARAMETERS ######\n",
    "\n",
    "task = \"Sex\"      # Options are \"Sex\", \"Age\", \"Expression\"\n",
    "network = \"vggD\"  # Options are \"vggC\", \"vggD\", \"vggE\", \"vggE1\"\n",
    "device = \"/cpu:0\" # \"/cpu:0\" or \"/gpu:0\"\n",
    "display_step = 1  # How often it prints out progress (1 = every epoch)\n",
    "saver_step = 10   # How often it saves the model (it will also save any with validation accuracy of higher than 84%)\n",
    "learning_rate = 0.0001 # I used learning rate = 0.0001\n",
    "\n",
    "######################################\n",
    "\n",
    "n_filters_conv1 = 64\n",
    "filter_size_conv1 = 3\n",
    "stride1 = 1\n",
    "\n",
    "n_filters_conv2 = 64\n",
    "filter_size_conv2 = 3\n",
    "stride2 = 1\n",
    "\n",
    "n_filters_conv3 = 128\n",
    "filter_size_conv3 = 3\n",
    "stride3 = 1\n",
    "\n",
    "n_filters_conv4 = 128\n",
    "filter_size_conv4 = 3\n",
    "stride4 = 1\n",
    "\n",
    "n_filters_conv5 = 256\n",
    "filter_size_conv5 = 3\n",
    "stride5 = 1\n",
    "\n",
    "n_filters_conv6 = 256\n",
    "filter_size_conv6 = 3\n",
    "stride6 = 1\n",
    "\n",
    "n_filters_conv7 = 256\n",
    "filter_size_conv7 = 3\n",
    "stride7 = 1\n",
    "if network == \"vggC\":\n",
    "    filter_size_conv7 = 1\n",
    "\n",
    "n_filters_conv75 = 256 ## used for vggE and vggE1 only\n",
    "filter_size_conv75 = 3\n",
    "stride75 = 1\n",
    "if network == \"vggE1\":\n",
    "    filter_size_conv75 = 1\n",
    "\n",
    "n_filters_conv8 = 512\n",
    "filter_size_conv8 = 3\n",
    "stride8 = 1\n",
    "\n",
    "n_filters_conv9 = 512\n",
    "filter_size_conv9 = 3\n",
    "stride9 = 1\n",
    "\n",
    "n_filters_conv10 = 512\n",
    "filter_size_conv10 = 3\n",
    "stride10 = 1\n",
    "if network == \"vggC\":\n",
    "    filter_size_conv10 = 1\n",
    "\n",
    "n_filters_conv105 = 512 ## used for vggE and vggE1 only\n",
    "filter_size_conv105 = 3\n",
    "stride105 = 1\n",
    "if network == \"vggE1\":\n",
    "    filter_size_conv105 = 1\n",
    "\n",
    "n_filters_conv11 = 512\n",
    "filter_size_conv11 = 3\n",
    "stride11 = 1\n",
    "\n",
    "n_filters_conv12 = 512\n",
    "filter_size_conv12 = 3\n",
    "stride12 = 1\n",
    "\n",
    "n_filters_conv13 = 512\n",
    "filter_size_conv13 = 3\n",
    "stride13 = 1\n",
    "if network == \"vggC\":\n",
    "    filter_size_conv13 = 1\n",
    "\n",
    "n_filters_conv135 = 512 ## used for vggE and vggE1 only\n",
    "filter_size_conv135 = 3\n",
    "stride135 = 1\n",
    "if network == \"vggE1\":\n",
    "    filter_size_conv135 = 1\n",
    "\n",
    "fc1_layer_size = 4096\n",
    "fc2_layer_size = 4096\n",
    "\n",
    "def make_one_hot(labels):\n",
    "    global n_classes\n",
    "    one_label = np.zeros(n_classes)\n",
    "    new_labels = [one_label] * len(labels)\n",
    "    for i in range(len(labels)):\n",
    "        one_label = np.zeros(n_classes)\n",
    "        one_label[int(labels[i])] = 1\n",
    "        new_labels[i] = one_label\n",
    "    return np.array(new_labels)\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data.reshape([-1, 128, 128, 1])  \n",
    "        self.labels = labels\n",
    "        self.batch_index = 0\n",
    "\n",
    "    def randomize(self, sess):\n",
    "        shuffled_data = np.empty(self.data.shape, dtype=self.data.dtype)\n",
    "        shuffled_labels = np.empty(self.labels.shape, dtype=self.labels.dtype)\n",
    "        permutation = np.random.permutation(len(self.data))\n",
    "        for old_index, new_index in enumerate(permutation):\n",
    "            shuffled_data[new_index] = self.data[old_index]\n",
    "            shuffled_labels[new_index] = self.labels[old_index]\n",
    "        self.data = shuffled_data\n",
    "        self.labels = shuffled_labels\n",
    "\n",
    "    def next_batch(self, b_size):\n",
    "        start = self.batch_index\n",
    "        end = self.batch_index + b_size\n",
    "        self.batch_index = end\n",
    "        return self.data[start:end], self.labels[start:end]\n",
    "\n",
    "\n",
    "def conv_relu_layer(input, n_input, n_filters, filter_size, stride):\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=[filter_size, filter_size, n_input, n_filters], stddev=0.05))\n",
    "    biases = tf.Variable(tf.constant(0.05, shape=[n_filters]))\n",
    "    conv_layer = tf.nn.conv2d(input=input, filter=weights, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    conv_layer += biases\n",
    "    c_r_layer = tf.nn.relu(conv_layer)\n",
    "    return c_r_layer\n",
    "\n",
    "def maxpool_relu_layer(input):\n",
    "    m_layer = tf.nn.max_pool(value=input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    m_r_layer = tf.nn.relu(m_layer)\n",
    "    return m_r_layer\n",
    "\n",
    "def flat_layer(input_layer):\n",
    "    shape = input_layer.get_shape()\n",
    "    num_features = shape[1:4].num_elements()\n",
    "    flat_layer = tf.reshape(input_layer, [-1, num_features])\n",
    "    return flat_layer\n",
    "\n",
    "def fc_layer(input, n_inputs, n_outputs, use_relu=True):\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=[n_inputs, n_outputs], stddev=0.05))\n",
    "    biases = tf.Variable(tf.constant(0.05, shape=[n_outputs]))\n",
    "    fc_layer = tf.matmul(input, weights) + biases\n",
    "    if use_relu:\n",
    "        fc_layer = tf.nn.relu(fc_layer)\n",
    "    return fc_layer\n",
    "\n",
    "\n",
    "if task == \"Sex\":\n",
    "    n_classes = 2\n",
    "    train_labels = make_one_hot(trainingSexLabels)\n",
    "    valid_labels = make_one_hot(validationSexLabels)\n",
    "    test_labels = make_one_hot(testingSexLabels)\n",
    "    train_data = Dataset(trainingFaces, train_labels)\n",
    "    valid_data = Dataset(validationFaces, valid_labels)\n",
    "    test_data = Dataset(testingFaces, test_labels)\n",
    "    model = \"sex-model\"\n",
    "elif task == \"Age\":\n",
    "    n_classes = 4\n",
    "    train_labels = make_one_hot(trainingAgeLabels)\n",
    "    valid_labels = make_one_hot(validationAgeLabels)\n",
    "    test_labels = make_one_hot(testingAgeLabels)\n",
    "    train_data = Dataset(trainingFaces, train_labels)\n",
    "    valid_data = Dataset(validationFaces, valid_labels)\n",
    "    test_data = Dataset(testingFaces, test_labels)\n",
    "    model = \"age-model\"\n",
    "elif task == \"Expression\":\n",
    "    n_classes = 2\n",
    "    train_labels = make_one_hot(trainingExpLabels)\n",
    "    valid_labels = make_one_hot(validationExpLabels)\n",
    "    test_labels = make_one_hot(testingExpLabels)\n",
    "    train_data = Dataset(trainingFaces, train_labels)\n",
    "    valid_data = Dataset(validationFaces, valid_labels)\n",
    "    test_data = Dataset(testingFaces, test_labels)\n",
    "    model = \"exp-model\"\n",
    "else:\n",
    "    print(\"Please set task to one of the three options\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "img_size = 128\n",
    "num_channels = 1  # greyscale\n",
    "\n",
    "with tf.device(device):\n",
    "    # set up VGG\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        X = tf.placeholder(tf.float32, shape=[None, img_size, img_size, num_channels], name='X')\n",
    "        y_true = tf.placeholder(tf.float32, shape=[None, n_classes], name='y_true')\n",
    "        y_true_class = tf.argmax(y_true, dimension=1)\n",
    "\n",
    "        conv1 = conv_relu_layer(input=X, n_input=num_channels, n_filters=n_filters_conv1,\n",
    "                                filter_size=filter_size_conv1, stride = stride1)\n",
    "        conv2 = conv_relu_layer(input=conv1, n_input=n_filters_conv1, n_filters=n_filters_conv2,\n",
    "                                filter_size=filter_size_conv2, stride = stride2)\n",
    "        max1 = maxpool_relu_layer(conv2)\n",
    "        conv3 = conv_relu_layer(input=max1, n_input=n_filters_conv2, n_filters=n_filters_conv3,\n",
    "                                filter_size=filter_size_conv3, stride=stride3)\n",
    "        conv4 = conv_relu_layer(input=conv3, n_input=n_filters_conv3, n_filters=n_filters_conv4,\n",
    "                                filter_size=filter_size_conv4, stride=stride4)\n",
    "        max2 = maxpool_relu_layer(conv4)\n",
    "        conv5 = conv_relu_layer(input=max2, n_input=n_filters_conv4, n_filters=n_filters_conv5,\n",
    "                                filter_size=filter_size_conv5, stride = stride5)\n",
    "        conv6 = conv_relu_layer(input=conv5, n_input=n_filters_conv5, n_filters=n_filters_conv6,\n",
    "                                filter_size=filter_size_conv6, stride = stride6)\n",
    "        conv7 = conv_relu_layer(input=conv6, n_input=n_filters_conv6, n_filters=n_filters_conv7,\n",
    "                                filter_size=filter_size_conv7, stride = stride7)\n",
    "        if network == \"vggE\" or network == \"vggE1\":\n",
    "            conv75 = conv_relu_layer(input=conv7, n_input=n_filters_conv7, n_filters=n_filters_conv75,\n",
    "                                     filter_size=filter_size_conv75, stride=stride75)\n",
    "            max3 = maxpool_relu_layer(conv75)\n",
    "        else:\n",
    "            max3 = maxpool_relu_layer(conv7)\n",
    "        conv8 = conv_relu_layer(input=max3, n_input=n_filters_conv7, n_filters=n_filters_conv8,\n",
    "                                filter_size=filter_size_conv8, stride=stride8)\n",
    "        conv9 = conv_relu_layer(input=conv8, n_input=n_filters_conv8, n_filters=n_filters_conv9,\n",
    "                                filter_size=filter_size_conv9, stride=stride9)\n",
    "        conv10 = conv_relu_layer(input=conv9, n_input=n_filters_conv9, n_filters=n_filters_conv10,\n",
    "                                filter_size=filter_size_conv10, stride=stride10)\n",
    "        if network == \"vggE\" or network == \"vggE1\":\n",
    "            conv105 = conv_relu_layer(input=conv10, n_input=n_filters_conv10, n_filters=n_filters_conv105,\n",
    "                                      filter_size=filter_size_conv105, stride=stride105)\n",
    "            max4 = maxpool_relu_layer(conv105)\n",
    "        else:\n",
    "            max4 = maxpool_relu_layer(conv10)\n",
    "        conv11 = conv_relu_layer(input=max4, n_input=n_filters_conv10, n_filters=n_filters_conv11,\n",
    "                                 filter_size=filter_size_conv11, stride=stride11)\n",
    "        conv12 = conv_relu_layer(input=conv11, n_input=n_filters_conv11, n_filters=n_filters_conv12,\n",
    "                                 filter_size=filter_size_conv12, stride=stride12)\n",
    "        conv13 = conv_relu_layer(input=conv12, n_input=n_filters_conv12, n_filters=n_filters_conv13,\n",
    "                                 filter_size=filter_size_conv13, stride=stride13)\n",
    "        if network == \"vggE\" or network == \"vggE1\":\n",
    "            conv135 = conv_relu_layer(input=conv13, n_input=n_filters_conv13, n_filters=n_filters_conv135,\n",
    "                                      filter_size=filter_size_conv135, stride=stride135)\n",
    "            max5 = maxpool_relu_layer(conv135)\n",
    "        else:\n",
    "            max5 = maxpool_relu_layer(conv13)\n",
    "        flat = flat_layer(max5)\n",
    "        fc1 = fc_layer(input=flat, n_inputs=flat.get_shape()[1:4].num_elements(), n_outputs=fc1_layer_size)\n",
    "        fc2 = fc_layer(input=fc1, n_inputs=fc1_layer_size, n_outputs=fc2_layer_size)\n",
    "        fc3 = fc_layer(input=fc2, n_inputs=fc2_layer_size, n_outputs=n_classes, use_relu=False)  \n",
    "        y_pred = tf.nn.softmax(fc3, name=\"y_pred\")\n",
    "        y_pred_class = tf.argmax(y_pred, dimension=1)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=fc3, labels=y_true) \n",
    "        cost = tf.reduce_mean(cross_entropy)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "        correct_prediction = tf.equal(y_pred_class, y_true_class)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "\n",
    "print(\"Graph initialised\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Details\n",
    "\n",
    "I randomize the training data at the beginning of each epoch, so that each image is not always seen as part of the same batch and in the same order. An epoch consists of one cycle of every training image (actually, two are missed every epoch because there are 19970 training images, which doesn't divide evenly into batches of 16). Cost is calculated as the mean of the cross-entropy between the output of the final layer of the network and the true labels. I used the Adam Optimizer [16], which adjusts the learning rate as it trains, enabling it to learn faster initially and then slow down the rate of training as it begins to converge on some minimum solution. I used a starting learning rate of 0.0001, because I tried some faster and some slower rates and this seemed to produce the best and most reliable results. Classification accuracy is computed as the percentage of correctly classified images. \n",
    "\n",
    "The next cell trains the network -- this takes a long time if not using a GPU, especially if using augmented (and thus 10 times more) training data. I've included the pre-trained models from which I derived my final (best) results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "####### MODIFY THESE PARAMETERS ###### (I used batch_size = 16)\n",
    "\n",
    "n_epochs = 200\n",
    "batch_size = 16\n",
    "\n",
    "######################################\n",
    "\n",
    "n_batches = trainingFaces.shape[0] // batch_size\n",
    "val_batches = validationFaces.shape[0] // batch_size\n",
    "\n",
    "with tf.device(device):\n",
    "    with g.as_default():    \n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            val_acc = 0\n",
    "            val_loss = 0\n",
    "            for val in range(val_batches):\n",
    "                x_valid_batch, y_valid_batch = valid_data.next_batch(batch_size)\n",
    "                feed_dict_val = {X: x_valid_batch, y_true: y_valid_batch}\n",
    "                val_acc += sess.run(accuracy, feed_dict=feed_dict_val)\n",
    "                val_loss += sess.run(cost, feed_dict=feed_dict_val)\n",
    "            val_acc = val_acc / val_batches\n",
    "            val_loss = val_loss / val_batches\n",
    "            msg = \"Pre-training (Epoch {0}) --- Training Accuracy: {1:>6.2%}, Validation Accuracy: {2:>6.2%},  Validation Loss: {3:.3f}\"\n",
    "            print(msg.format(0, 0, val_acc, val_loss))  # , val_loss))\n",
    "            sys.stdout.flush()\n",
    "            for i in range(1, n_epochs + 1):\n",
    "                train_data.randomize(sess)\n",
    "                train_data.batch_index = 0\n",
    "                valid_data.randomize(sess)\n",
    "                valid_data.batch_index = 0\n",
    "                acc = 0\n",
    "                val_acc = 0\n",
    "                val_loss = 0\n",
    "                for batch in range(n_batches):\n",
    "                    x_batch, y_true_batch = train_data.next_batch(batch_size)\n",
    "                    feed_dict_train = {X: x_batch, y_true: y_true_batch}\n",
    "                    sess.run(optimizer, feed_dict=feed_dict_train)\n",
    "                    acc += sess.run(accuracy, feed_dict=feed_dict_train)\n",
    "                if i % display_step == 0:\n",
    "                    valid_data.batch_index = 0\n",
    "                    for j in range(val_batches):\n",
    "                        x_valid_batch, y_valid_batch = valid_data.next_batch(batch_size)\n",
    "                        feed_dict_val = {X: x_valid_batch, y_true: y_valid_batch}\n",
    "                        val_acc += sess.run(accuracy, feed_dict=feed_dict_val)\n",
    "                        val_loss += sess.run(cost, feed_dict=feed_dict_val)\n",
    "                acc = acc / n_batches\n",
    "                val_acc = val_acc / val_batches\n",
    "                val_loss = val_loss / val_batches\n",
    "                msg = \"Training Epoch {0} --- Training Accuracy: {1:>6.2%}, Validation Accuracy: {2:>6.2%},  Validation Loss: {3:.3f}\"\n",
    "                print(msg.format(i, acc, val_acc, val_loss))\n",
    "                sys.stdout.flush()\n",
    "                if i % saver_step == 0 or val_acc > 0.84:\n",
    "                    save_path = saver.save(sess, model+\"_\"+network+\"_\"+str(i))\n",
    "                    \n",
    "print(\"Training done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on this dataset without augmentation\n",
    "\n",
    "The classification accuracy on testing data on the three tasks and on the four network architectures, trained on the original (non-augmented) training data, is given below. I've also included the number of epochs needed to reach that level of accuracy (after that, they tend to overtrain, and thus decrease a little in accuracy). Recall that we are trying to improve on 50% to beat the random classifier for the Sex task, and 61.4% to beat the most-common-class classifier. For the Age task, we need to improve on 25% and 88.7% respectively, and for the Expression task we need to improve on 50% and 39.9%.\n",
    "\n",
    "<table>\n",
    "<tr><th>Sex </th><th>Age</th><th>Expression</th></tr>\n",
    "<tr><td>\n",
    "\n",
    "|   Network   | VGG-C | VGG-D | VGG-E1 | VGG-E |\n",
    "| ------ |--------| -----|----- | ----- |\n",
    "| Accuracy | 83.27% | 83.37% | 82.06% | 83.47% |\n",
    "| Epochs | 70 | 90 | 120 | 120 |\n",
    "\n",
    "</td><td>\n",
    "\n",
    "|    Network  | VGG-C | VGG-D | VGG-E1 | VGG-E |\n",
    "| ------ |--------| -----|----- | ----- |\n",
    "| Accuracy | 90.22% | 91.73% | 90.42% | 91.53% |\n",
    "| Epochs | 70 | 100 | 190 | 80 |\n",
    "\n",
    "</td><td>\n",
    "\n",
    "| Network| VGG-C | VGG-D | VGG-E1 | VGG-E |\n",
    "| ------ |--------| -----|----- | ----- |\n",
    "| Accuracy | 83.47% | 85.79% | 84.48% | 84.48% |\n",
    "| Epochs | 40 | 100 | 90 | 90 |\n",
    "\n",
    "</td></tr> </table>\n",
    "\n",
    "All of the networks improve on both naive classifiers, but it is only minimally in the case of the Age task. VGG-D and VGG-E are the best performers, which is unsurprising given that they have become the famous VGG-16 and VGG-19, respectively (as VGG-D has 16 layers of trainable weights, and VGG-E has 19). VGG-C tends to peak more quickly, but never reach the same levels.\n",
    "\n",
    "## Results on this dataset with augmentation\n",
    "\n",
    "Because I judged VGG-D and VGG-E to be overall the best performers (the most accurate), I restricted my second investigation to these two architectures. The classifcation accuracy on testing data on these two networks, trained with augmented data, is given below.\n",
    "\n",
    "<table>\n",
    "<tr><th>Sex </th><th>Age</th><th>Expression</th></tr>\n",
    "<tr><td>\n",
    "\n",
    "| Network     | VGG-D |  VGG-E |\n",
    "| ------ |-----| ----- |\n",
    "| Accuracy | 88.91% | 82.06% |\n",
    "| Epochs |  70 | 30 |\n",
    "\n",
    "</td><td>\n",
    "\n",
    "| Network     |  VGG-D| VGG-E |\n",
    "| ------ |--------| ----- |\n",
    "| Accuracy |  90.22% | 89.92% |\n",
    "| Epochs |  20 | 70 |\n",
    "\n",
    "</td><td>\n",
    "\n",
    "| Network|  VGG-D | VGG-E |\n",
    "| ------ |--------| -----|\n",
    "| Accuracy |  85.89% | 84.07% |\n",
    "| Epochs |  40 | 20 |\n",
    "\n",
    "</td></tr> </table>\n",
    "\n",
    "Accuracy on the Sex task is the only one that significantly improved with data augmentation. Accuracy on the Expression task improved by a negligible amount, and accuracy on the Age task declined a little.\n",
    "\n",
    "So, the final results of the best version of each of my three classifiers (these models are included in my submission) produce the following results:\n",
    "\n",
    "| Task| Sex | Age | Expression |\n",
    "| ------ |--------| -----| -----|\n",
    "| Accuracy | 88.91%  | 91.73% | 85.89% |\n",
    "\n",
    "The next cell loads the trained models and computes the labels for the testing data on each of the three classification tasks. If the number of images in the training set is not divisible by 10, then they won't all obtain a prediction. However, 1000 images don't all fit into memory with a network this size, so obtaining a vector (numpy array) of all predictions at once must be done in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### These are the models I trained (I picked the best I got for each task), included in the zip file. \n",
    "#### You can the names and types to check those you've trained yourself.\n",
    "\n",
    "sexModel = \"best_sex-model_vggD\"\n",
    "sexType = \"vggD\"\n",
    "ageModel = \"best_age-model_vggD\"\n",
    "ageType = \"vggD\"\n",
    "expModel = \"best_exp-model_vggD\"\n",
    "expType = \"vggD\"\n",
    "\n",
    "def predictLabels(data, n_data, model, taskIn, networkIn):\n",
    "    batch_size = 10 # This assumes the number of images in the training set is divisible by 10 \n",
    "    task = taskIn  # Options are \"Sex\", \"Age\", \"Expression\"\n",
    "    network = networkIn # Options are \"vggC\", \"vggD\", \"vggE\", \"vggE1\"\n",
    "    device = \"/cpu:0\" # \"/cpu:0\" or \"/gpu:0\"\n",
    "\n",
    "    n_filters_conv1 = 64\n",
    "    filter_size_conv1 = 3\n",
    "    stride1 = 1\n",
    "\n",
    "    n_filters_conv2 = 64\n",
    "    filter_size_conv2 = 3\n",
    "    stride2 = 1\n",
    "\n",
    "    n_filters_conv3 = 128\n",
    "    filter_size_conv3 = 3\n",
    "    stride3 = 1\n",
    "\n",
    "    n_filters_conv4 = 128\n",
    "    filter_size_conv4 = 3\n",
    "    stride4 = 1\n",
    "\n",
    "    n_filters_conv5 = 256\n",
    "    filter_size_conv5 = 3\n",
    "    stride5 = 1\n",
    "\n",
    "    n_filters_conv6 = 256\n",
    "    filter_size_conv6 = 3\n",
    "    stride6 = 1\n",
    "\n",
    "    n_filters_conv7 = 256\n",
    "    filter_size_conv7 = 3\n",
    "    stride7 = 1\n",
    "    if network == \"vggC\":\n",
    "        filter_size_conv7 = 1\n",
    "\n",
    "    n_filters_conv75 = 256 ## used for vggE and vggE1 only\n",
    "    filter_size_conv75 = 3\n",
    "    stride75 = 1\n",
    "    if network == \"vggE1\":\n",
    "        filter_size_conv75 = 1\n",
    "\n",
    "    n_filters_conv8 = 512\n",
    "    filter_size_conv8 = 3\n",
    "    stride8 = 1\n",
    "\n",
    "    n_filters_conv9 = 512\n",
    "    filter_size_conv9 = 3\n",
    "    stride9 = 1\n",
    "\n",
    "    n_filters_conv10 = 512\n",
    "    filter_size_conv10 = 3\n",
    "    stride10 = 1\n",
    "    if network == \"vggC\":\n",
    "        filter_size_conv10 = 1\n",
    "\n",
    "    n_filters_conv105 = 512 ## used for vggE and vggE1 only\n",
    "    filter_size_conv105 = 3\n",
    "    stride105 = 1\n",
    "    if network == \"vggE1\":\n",
    "        filter_size_conv105 = 1\n",
    "\n",
    "    n_filters_conv11 = 512\n",
    "    filter_size_conv11 = 3\n",
    "    stride11 = 1\n",
    "\n",
    "    n_filters_conv12 = 512\n",
    "    filter_size_conv12 = 3\n",
    "    stride12 = 1\n",
    "\n",
    "    n_filters_conv13 = 512\n",
    "    filter_size_conv13 = 3\n",
    "    stride13 = 1\n",
    "    if network == \"vggC\":\n",
    "        filter_size_conv13 = 1\n",
    "\n",
    "    n_filters_conv135 = 512 ## used for vggE and vggE1 only\n",
    "    filter_size_conv135 = 3\n",
    "    stride135 = 1\n",
    "    if network == \"vggE1\":\n",
    "        filter_size_conv135 = 1\n",
    "\n",
    "    fc1_layer_size = 4096\n",
    "    fc2_layer_size = 4096\n",
    "\n",
    "    if task == \"Sex\" or task == \"Expression\":\n",
    "        n_classes = 2\n",
    "    elif task == \"Age\":\n",
    "        n_classes = 4\n",
    "    else:\n",
    "        print(\"Please set task to one of the three options\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    img_size = 128\n",
    "    num_channels = 1  # greyscale\n",
    "    \n",
    "    with tf.device(device):\n",
    "        # set up VGG\n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            X = tf.placeholder(tf.float32, shape=[None, img_size, img_size, num_channels], name='X')\n",
    "            y_true = tf.placeholder(tf.float32, shape=[None, n_classes], name='y_true')\n",
    "            y_true_class = tf.argmax(y_true, dimension=1)\n",
    "\n",
    "            conv1 = conv_relu_layer(input=X, n_input=num_channels, n_filters=n_filters_conv1,\n",
    "                                    filter_size=filter_size_conv1, stride = stride1)\n",
    "            conv2 = conv_relu_layer(input=conv1, n_input=n_filters_conv1, n_filters=n_filters_conv2,\n",
    "                                    filter_size=filter_size_conv2, stride = stride2)\n",
    "            max1 = maxpool_relu_layer(conv2)\n",
    "            conv3 = conv_relu_layer(input=max1, n_input=n_filters_conv2, n_filters=n_filters_conv3,\n",
    "                                    filter_size=filter_size_conv3, stride=stride3)\n",
    "            conv4 = conv_relu_layer(input=conv3, n_input=n_filters_conv3, n_filters=n_filters_conv4,\n",
    "                                    filter_size=filter_size_conv4, stride=stride4)\n",
    "            max2 = maxpool_relu_layer(conv4)\n",
    "            conv5 = conv_relu_layer(input=max2, n_input=n_filters_conv4, n_filters=n_filters_conv5,\n",
    "                                    filter_size=filter_size_conv5, stride = stride5)\n",
    "            conv6 = conv_relu_layer(input=conv5, n_input=n_filters_conv5, n_filters=n_filters_conv6,\n",
    "                                    filter_size=filter_size_conv6, stride = stride6)\n",
    "            conv7 = conv_relu_layer(input=conv6, n_input=n_filters_conv6, n_filters=n_filters_conv7,\n",
    "                                    filter_size=filter_size_conv7, stride = stride7)\n",
    "            if network == \"vggE\" or network == \"vggE1\":\n",
    "                conv75 = conv_relu_layer(input=conv7, n_input=n_filters_conv7, n_filters=n_filters_conv75,\n",
    "                                         filter_size=filter_size_conv75, stride=stride75)\n",
    "                max3 = maxpool_relu_layer(conv75)\n",
    "            else:\n",
    "                max3 = maxpool_relu_layer(conv7)\n",
    "            conv8 = conv_relu_layer(input=max3, n_input=n_filters_conv7, n_filters=n_filters_conv8,\n",
    "                                    filter_size=filter_size_conv8, stride=stride8)\n",
    "            conv9 = conv_relu_layer(input=conv8, n_input=n_filters_conv8, n_filters=n_filters_conv9,\n",
    "                                    filter_size=filter_size_conv9, stride=stride9)\n",
    "            conv10 = conv_relu_layer(input=conv9, n_input=n_filters_conv9, n_filters=n_filters_conv10,\n",
    "                                    filter_size=filter_size_conv10, stride=stride10)\n",
    "            if network == \"vggE\" or network == \"vggE1\":\n",
    "                conv105 = conv_relu_layer(input=conv10, n_input=n_filters_conv10, n_filters=n_filters_conv105,\n",
    "                                          filter_size=filter_size_conv105, stride=stride105)\n",
    "                max4 = maxpool_relu_layer(conv105)\n",
    "            else:\n",
    "                max4 = maxpool_relu_layer(conv10)\n",
    "            conv11 = conv_relu_layer(input=max4, n_input=n_filters_conv10, n_filters=n_filters_conv11,\n",
    "                                     filter_size=filter_size_conv11, stride=stride11)\n",
    "            conv12 = conv_relu_layer(input=conv11, n_input=n_filters_conv11, n_filters=n_filters_conv12,\n",
    "                                     filter_size=filter_size_conv12, stride=stride12)\n",
    "            conv13 = conv_relu_layer(input=conv12, n_input=n_filters_conv12, n_filters=n_filters_conv13,\n",
    "                                     filter_size=filter_size_conv13, stride=stride13)\n",
    "            if network == \"vggE\" or network == \"vggE1\":\n",
    "                conv135 = conv_relu_layer(input=conv13, n_input=n_filters_conv13, n_filters=n_filters_conv135,\n",
    "                                          filter_size=filter_size_conv135, stride=stride135)\n",
    "                max5 = maxpool_relu_layer(conv135)\n",
    "            else:\n",
    "                max5 = maxpool_relu_layer(conv13)\n",
    "            flat = flat_layer(max5)\n",
    "            fc1 = fc_layer(input=flat, n_inputs=flat.get_shape()[1:4].num_elements(), n_outputs=fc1_layer_size)\n",
    "            #fc1 = fc_layer(input=max5, n_inputs=filter_size_conv13, n_outputs=fc1_layer_size)\n",
    "            fc2 = fc_layer(input=fc1, n_inputs=fc1_layer_size, n_outputs=fc2_layer_size)\n",
    "            fc3 = fc_layer(input=fc2, n_inputs=fc2_layer_size, n_outputs=n_classes, use_relu=False)  # n_outputs=n_classes\n",
    "            y_pred = tf.nn.softmax(fc3, name=\"y_pred\")\n",
    "            y_pred_class = tf.argmax(y_pred, dimension=1, name=\"y_pred_class\")\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=fc3, labels=y_true)\n",
    "            cost = tf.reduce_mean(cross_entropy)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "            correct_prediction = tf.equal(y_pred_class, y_true_class)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "            \n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                saver.restore(sess, model)\n",
    "                n_batches = n_data//batch_size\n",
    "                predicted_labels = np.array([])\n",
    "                for i in range(n_batches):\n",
    "                    x_test, y_test = data.next_batch(batch_size)\n",
    "                    feed_dict_val = {X: x_test, y_true: y_test}\n",
    "                    predicted_labels = np.append(predicted_labels, sess.run(y_pred_class, feed_dict=feed_dict_val))\n",
    "            return predicted_labels\n",
    "    \n",
    "# This function will be used to evaluate your submission.\n",
    "def computeLabels(faceData):\n",
    "    n, d = faceData.shape\n",
    "    # Zero arrays for the labels, should be able to do better than this\n",
    "    estSexLabels = np.zeros(n)\n",
    "    estAgeLabels = np.zeros(n)\n",
    "    estExpLabels = np.zeros(n)\n",
    "    \n",
    "    # turn faceData into a Dataset object\n",
    "    sex_labels = np.array([[0]*2]*n)\n",
    "    dataset = Dataset(faceData, sex_labels)\n",
    "    estSexLabels = predictLabels(data=dataset, n_data=n, model=sexModel, taskIn=\"Sex\", networkIn=sexType)\n",
    "\n",
    "    age_labels = np.array([[0]*4]*n)\n",
    "    dataset = Dataset(faceData, age_labels)\n",
    "    estAgeLabels = predictLabels(data=dataset, n_data=n, model=ageModel, taskIn=\"Age\", networkIn=ageType)\n",
    "\n",
    "    exp_labels = np.array([[0]*2]*n)\n",
    "    dataset = Dataset(faceData, exp_labels)\n",
    "    estExpLabels = predictLabels(data=dataset, n_data=n, model=expModel, taskIn=\"Expression\", networkIn=expType)\n",
    "    \n",
    "    return estSexLabels, estAgeLabels, estExpLabels\n",
    "\n",
    "estS, estA, estE = computeLabels(validationFaces)\n",
    "# print(estS.shape, estA.shape, estE.shape)\n",
    "# I'll do stuff with the above to evaluate the accuracy of your methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The performance on the four variations of VGGNet was similar, and without doing several trials to get a measure of the average performance and number of epochs to converge on a solution, I wouldn't claim any general conclusions about their relative performance. Data augmentation significantly improved performance on the Sex task, but not on the other two tasks. It did, however, get them to converge in far fewer epochs, though of course each epoch lets it learn from ten times as many images, so that is unsurprising. Comparing to the naive classifiers, my versions of VGGNet outperformed them both significantly on the Sex and Expression tasks. However, on the Age task, I only obtained a best accuracy of 91.73%, which (while significantly better than the random classifier) is a mere 3% better than the hypothetical classifier that learns the most common class and predicts every image is in that one. Perhaps this task is more difficult to learn, because of the heavy bias toward the 'adult' class. Because predicting that everyone is an adult gives such high accuracy (and, more relevantly for training, such low cost), it is difficult to leap out of this deep local minimum. Perhaps, also, augmenting the data in the uniform way I chose to do it serves to entrench the bias toward the significantly more popular class (though this wouldn't explain the lack of improvement on the Expression task). If this is the case, it could perhaps be mitigated by increasing the number of training examples of the other three classes but not those in the popular class.\n",
    "\n",
    "In every case, though, the network easily reached 100% accuracy on training data. So, perhaps it is just the case that it is highly subject to overtraining, and the accuracy reflects how similar the testing data is to the training data. A network that has memorised the training data for the Age task will have a heavy bias toward predicting 'adult' for any given image, and if we add the supposition that some significant proportion of the testing images look sufficiently similar to one of the training images as to produce the correct label just through finding the closest match (rather than learning semantic features relevant to classifying this feature), then it is plausible that this would account for the apparantly high accuracy of 91.73%.\n",
    "\n",
    "I made one last attempt to improve the performance of the classifiers: I introduced dropout to each max-pooling layer (this can be seen at [17]). Dropout is a method where, during training, the activations of some proportion of the neurons in each layer are set to zero (selected at random for each batch). This has the effect of ensuring that the network can't rely on all the features being available to provide information, so it is forced to learn general features that apply to more images (in other words, it prevents overtraining). I tried it with dropout rates of 0.9 (only 10% of units remain in play), 0.5, and 0.25, but none of them helped. They all slowed training down significantly, but the peak accuracy of the networks was much lower than without dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "\n",
    "[1] A. Krizhevsky, I. Sutskever, and G. Hinton. \"ImageNet Classification with Deep Convolutional Neural Networks.\" NIPS, 1097--1105, 2012.\n",
    "\n",
    "[2] K. Simonyan and A. Zisserman. \"Very Deep Convolutional Networks for Large-Scale Image Recognition.\" ICLR, 2015.\n",
    "\n",
    "[3] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. \"Going Deeper with Convolutions.\" CoRR, 2014.\n",
    "\n",
    "[4] K. He, X. Zhang, S. Ren, and J. Sun. \"Deep Residual Learning for Image Recognition.\" CoRR, 2015.\n",
    "\n",
    "[5] W. S. McCulloch and W. Pitts. \"A Logical Calculus of the Ideas Immanent in Nervous Activity.\" Bulletin of Mathematical Biophysics, 5: 115--133, 1943.\n",
    "\n",
    "[6] D. O. Hebb. \"The Organisation of Behaviour: a Neuropsychological Theory.\" John Wiley \\& Sons, NY, 1949.\n",
    "\n",
    "[7] F. Rosenblatt. \"Principles of Neurodynamics.\" Spartan, NY, 1962.\n",
    "\n",
    "[8] M. Minsky and S. Papert. \"Perceptrons.\" MIT Press, Cambridge, MA, 1969.\n",
    "\n",
    "[9] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. \"Learning Internal Representations by Error Propagation.\" In D. E. Rumelhart, J. L. McClelland, The PDP Research Group (eds.) \"Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations.\" MIT Press, Cambridge, MA, 1986.\n",
    "\n",
    "[10] Y. LeCun, Y. Bengio, and G. Hinton. \"Deep Learning.\" Nature, 521(7553): 436--444, 2015.\n",
    "\n",
    "[11] Z. Qawaqneh, A. A. Mallouh, and B. D. Barkana. \"Deep Convolutional Neural Network for Age Estimation based on VGG-Face Model.\" CoRR, 2017.\n",
    "\n",
    "[12] O. Arriaga, M. Valdenegro-Toro, and P. Plöger. \"Real-time Convolutional Neural Networks for Emotion and Gender Classification.\" Corr, 2017.\n",
    "\n",
    "[13] https://github.com/hannahcy/face-recognition/blob/master/trainer_resnet.py\n",
    "\n",
    "[14] https://medium.com/@thimblot/data-augmentation-boost-your-image-dataset-with-few-lines-of-python-155c2dc1baec\n",
    "\n",
    "[15] M. Lin, Q. Chen, and S. Yan \"Network In Network.\" CoRR, 2013.\n",
    "\n",
    "[16] D. P. Kingma and J. Ba. \"Adam: A Method for Stochastic Optimization.\" ICLR, 2015.\n",
    "\n",
    "[17] https://github.com/hannahcy/face-recognition/blob/master/trainer_vgg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
